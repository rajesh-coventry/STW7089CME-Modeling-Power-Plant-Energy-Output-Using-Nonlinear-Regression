{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba87e67f",
   "metadata": {},
   "source": [
    "# **Must Know Concepts:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba39659",
   "metadata": {},
   "source": [
    "### **1. Ordinary Least Squares (OLS)**:\n",
    "\n",
    "OLS is a method for finding the best-fitting linear relationship between the input features (**X**) and the output/target variable (**y**) by **minimizing the sum of squared errors** (also called residuals):\n",
    "\n",
    "> $\\text{Loss (Cost)} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "Where:\n",
    "* $y_i$ is the actual target value.\n",
    "* $\\hat{y}_i = X_i w$ is the predicted value.\n",
    "* $w$ are the weights (coefficients) we want to find.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Closed-Form Solution / Normal Equation:**\n",
    "\n",
    "The **closed-form solution** to the OLS problem gives a direct formula to calculate the optimal weights:\n",
    "\n",
    "> $$\\hat{w} = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "This is called the **normal equation**. It's derived from setting the gradient of the OLS cost function to zero and solving for $w$.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Limitations of Closed-Form Solution:**\n",
    "\n",
    "* **Computationally expensive** for large feature sets due to matrix inversion $(X^T X)^{-1}$.\n",
    "\n",
    "* Often replaced by **`iterative methods`** like **`gradient descent`** when $X$ is large or sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a385d2e3",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **4. RSS (Residual Sum of Squares) in Regression:**\n",
    "\n",
    "In **regression analysis**, especially **linear regression**, the **Residual Sum of Squares (RSS)** is a measure of **how well the regression line fits the data**. It tells us **how much error** there is between the actual data points and the predicted values from the model.\n",
    "\n",
    "1. **You have data points** — real observations with inputs (like house size) and outputs (like house price).\n",
    "\n",
    "2. **You create a model** — say, a straight line to predict price based on size.\n",
    "\n",
    "3. **You predict** the outputs using your model — these are the **predicted values**.\n",
    "\n",
    "4. **You compare** these predicted values with the actual values — the **difference** is called the **residual** (or error).\n",
    "\n",
    "**What is a Residual?**\n",
    "\n",
    "A **residual** is the difference between the **actual value** and the **predicted value**:\n",
    "\n",
    "> $$\\text{Residual} = \\text{Actual value} - \\text{Predicted value}$$\n",
    "\n",
    "Sometimes residuals are positive (the model underpredicted), and sometimes negative (the model overpredicted).\n",
    "\n",
    "**Why Square the Residuals?**\n",
    "\n",
    "* If we just added up the residuals, positive and negative errors might cancel each other out.\n",
    "\n",
    "* So, we **square** each residual to make all values positive and emphasize larger errors more.\n",
    "\n",
    "* This gives us the **squared residuals**.\n",
    "\n",
    "**What is RSS?**\n",
    "\n",
    "The **Residual Sum of Squares (RSS)** is the **sum of all squared residuals**:\n",
    "\n",
    "> $$\\text{RSS} = \\sum (\\text{Actual} - \\text{Predicted})^2$$\n",
    "\n",
    "In words:\n",
    "\n",
    "> \"RSS measures the **total squared difference** between what the model predicts and what the actual data values are.\"\n",
    "\n",
    "**Why is RSS Important?**\n",
    "\n",
    "* **Lower RSS** means the model fits the data **better** (less error).\n",
    "* **Higher RSS** means the model fits the data **worse** (more error).\n",
    "* It’s the **key quantity that OLS (Ordinary Least Squares)** tries to **minimize** when finding the best-fitting line.\n",
    "\n",
    "Imagine you're trying to fit a line through a scatterplot of points. From each point, draw a vertical line to the regression line — that’s the **residual**. The RSS is the **sum of the squares of all those vertical lines**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb0c51e",
   "metadata": {},
   "source": [
    "### **5. What Is Likelihood?**\n",
    "\n",
    "In **regression analysis**, especially when using **`probabilistic or statistical models`** like **`linear regression`** or **`logistic regression`**, the concept of **`log-likelihood`** plays a key role in estimating the model parameters.\n",
    "\n",
    "Let’s say you build a model (like a line in linear regression), and you want to know:\n",
    "\n",
    "**How likely is it that this model would produce the data you actually observed?**\n",
    "\n",
    "This \"likelihood\" is a number that tells you how *`probable`* your data is, **given** your model’s parameters.\n",
    "\n",
    "**What Is the Likelihood Function?**\n",
    "\n",
    "The **likelihood function** is a mathematical function that:\n",
    "\n",
    "* Takes the model’s parameters as input (like the weights in a regression line),\n",
    "* And tells you how likely the **`observed data`** is under that model.\n",
    "\n",
    "In regression, this is often based on the assumption that errors (residuals) follow a **`normal distribution`** — meaning most predictions are close to the true value, and big errors are rare.\n",
    "\n",
    "**Why Use the `Log` of the Likelihood?**\n",
    "\n",
    "Working with raw likelihood values directly can be tricky because:\n",
    "\n",
    "* The likelihood often involves multiplying many small probabilities together, which can become **very tiny numbers** (leading to **underflow**).\n",
    "\n",
    "* Taking the **logarithm** (log) of the likelihood turns the multiplication into **addition**, which is easier to work with mathematically.\n",
    "\n",
    "This gives us the **log-likelihood function**, which is just the logarithm of the likelihood function.\n",
    "\n",
    "**What Is the Log-Likelihood Function?**\n",
    "\n",
    "The **log-likelihood function** measures how well a model explains the observed data, **`in logarithmic form`**.\n",
    "\n",
    "It’s a function of the model’s parameters — so we can use it to **find the best parameters** by **maximizing** the log-likelihood.\n",
    "\n",
    "This approach is called **Maximum Likelihood Estimation (MLE)**.\n",
    "\n",
    "**Key Use:**\n",
    "\n",
    "In regression (especially logistic regression), we often **`maximize the log-likelihood`** to find the **`best-fitting parameters`**, just like we minimize RSS in OLS linear regression.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596cfb21",
   "metadata": {},
   "source": [
    "### **6. AIC and BIC:**\n",
    "\n",
    "In **regression analysis**, especially when comparing different models, it’s important not only to find one that fits the data well, but also to **`avoid overfitting`**. That’s where **AIC** and **BIC** come in.\n",
    "\n",
    "* **`AIC`** stands for **`Akaike Information Criterion`**.\n",
    "\n",
    "* **`BIC`** stands for **`Bayesian Information Criterion`**.\n",
    "\n",
    "Both are tools to help you **compare models** and **choose the best one** by balancing two things:\n",
    "\n",
    "1. **Goodness of fit** – how well the model explains the data.\n",
    "\n",
    "2. **Model complexity** – how many parameters (e.g., features, coefficients) the model uses.\n",
    "\n",
    "**`The key idea:`** A model that fits really well *but is very complex* might be overfitting. `AIC` and `BIC` try to prevent that.\n",
    "\n",
    "**Why We Need:**\n",
    "\n",
    "Imagine you’re comparing two regression models:\n",
    "\n",
    "* Model A has 2 parameters.\n",
    "* Model B has 10 parameters.\n",
    "\n",
    "Model B might fit the data **better**, but it also might be **memorizing** the data rather than **generalizing**. AIC and BIC help decide **if the better fit is worth the extra complexity**.\n",
    "\n",
    "**The Basic Formula:**\n",
    "\n",
    "Both AIC and BIC are based on the **`log-likelihood`** (how likely the model is to produce the observed data), and both add a **`penalty`** for the number of parameters.\n",
    "\n",
    ">  **AIC = -2 × log-likelihood + 2 × (number of parameters)**\n",
    "\n",
    ">  **BIC = -2 × log-likelihood + ln(n) × (number of parameters)**\n",
    "\n",
    "Where:\n",
    "* **`log-likelihood`** rewards good fit.\n",
    "* **`number of parameters`** punishes complexity.\n",
    "* **`n`** is the number of observations (for BIC only).\n",
    "\n",
    "**What Do the Numbers Mean?**\n",
    "\n",
    "* **Lower AIC or BIC = Better model**\n",
    "\n",
    "* These numbers are **`relative`**, not absolute. You use them to **`compare models`**, not to say whether one model is “good” in isolation.\n",
    "\n",
    "---\n",
    "\n",
    "* **AIC** is more forgiving — it allows slightly more complex models if they improve fit.\n",
    "\n",
    "* **BIC** is stricter — it favors simpler models unless the complex one offers a *much* better fit.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Imagine you're fitting polynomial regression (curved lines) with degrees 1 to 5:\n",
    "\n",
    "| Model (degree)           | AIC | BIC |\n",
    "| ------------------------ | --- | --- |\n",
    "| Degree 1 (straight line) | 150 | 155 |\n",
    "| Degree 2                 | 140 | 148 |\n",
    "| Degree 3                 | 135 | 145 |\n",
    "| Degree 4                 | 134 | 147 |\n",
    "| Degree 5                 | 133 | 150 |\n",
    "\n",
    "* AIC might suggest degree 5 is best (lowest AIC).\n",
    "* BIC might suggest degree 3 is better (because the penalty is stronger and 133 → 150 is not worth the complexity).\n",
    "\n",
    "* Use **AIC** when your goal is **prediction accuracy** (e.g., in machine learning).\n",
    "\n",
    "* Use **BIC** when you're looking for the **true underlying model** (e.g., in scientific modeling).\n",
    "\n",
    "* Both are great tools when you're **comparing different models** with different numbers of parameters.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12a113",
   "metadata": {},
   "source": [
    "### **7. Residual's Distribution:**\n",
    "\n",
    "A **residual** is the **error** or **difference** between:\n",
    "\n",
    "* The **actual value** (what really happened), and\n",
    "* The **predicted value** (what the regression model says should happen).\n",
    "\n",
    "> $$\\text{Residual} = \\text{Actual Value} - \\text{Predicted Value}$$\n",
    "\n",
    "So if your model predicts that a house price is \\$300,000 but the real price is \\$320,000, the residual is:\n",
    "\n",
    "> $$\\text{Residual} = 320,000 - 300,000 = +20,000$$\n",
    "\n",
    "**What Is the Residuals Distribution?**\n",
    "\n",
    "When you make predictions for **many data points**, you'll get **many residuals** — one for each prediction.\n",
    "\n",
    "The **residuals distribution** is the overall **pattern** or **spread** of these residuals.\n",
    "\n",
    "In a **good regression model**, the residuals should behave in a certain way — they should look like **random noise** around zero.\n",
    "\n",
    "**Why Is the Residuals Distribution Important?**\n",
    "\n",
    "Studying the residuals helps us know if our model is:\n",
    "\n",
    "* Making consistent errors\n",
    "* Missing patterns in the data\n",
    "* Violating basic regression assumptions\n",
    "\n",
    "**Properties of a Good Residuals Distribution:**\n",
    "\n",
    "When your regression model is **well-fitted and valid**, the residuals should have the following **ideal properties**:\n",
    "\n",
    "1. **The Mean of Residuals Is Zero:**  \n",
    "* On average, the residuals should cancel out.\n",
    "* Some errors will be positive (underprediction), and some will be negative (overprediction).\n",
    "* The average of all residuals should be **close to 0**.\n",
    "\n",
    "2. **Residuals Are Randomly Scattered**\n",
    "* When you plot residuals against predicted values or inputs, they should appear as **a random cloud**, not a pattern.\n",
    "* If you see patterns (like curves or funnels), it means the model **missed something** — like a missing variable or nonlinearity.\n",
    "\n",
    "3. **Constant Variance (Homoscedasticity)**\n",
    "* The **spread of residuals** should stay about the same for all levels of the predicted values.\n",
    "* If the residuals get wider or narrower (a \"funnel shape\"), it’s called **heteroscedasticity**, which is a problem.\n",
    "\n",
    "4. **Residuals Are Normally Distributed (Optional but Helpful)**\n",
    "* If you're using **linear regression** and want to do **inference** (like confidence intervals or hypothesis tests), it helps if residuals follow a **normal (bell-shaped) distribution**.\n",
    "* This doesn’t have to be perfect, but large deviations from normality might affect your conclusions.\n",
    "\n",
    "5. **No Autocorrelation in Residuals**\n",
    "* Especially in **time series** or sequential data, residuals shouldn’t be correlated with one another.\n",
    "* If they are, it means there's some pattern the model didn't capture.\n",
    "\n",
    "> Residuals are like your model’s “mistakes” — and **how those mistakes behave tells you a lot** about whether your model is smart or just guessing wrong in a patterned way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd14c65",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
